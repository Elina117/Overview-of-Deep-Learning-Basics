{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Напишите функцию get_normalize, которая будет принимать тензор с признаками объектов из какого-то датасета с картинками и будет возвращать поканальное среднее и поканальное стандартное отклонение. Гарантируется, что матрица будет иметь размер [N, C, H, W], где N это количество объектов, C — количество каналов, H, W — размеры изображений. Нужно вернуть кортеж из двух тензоров длины C.\n\nВаша функция должна иметь следующую сигнатуру def get_normalize(features: torch.Tensor):","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef get_normalize(features: torch.Tensor):\n    mean = features.mean(dim=(0, 2, 3))\n    std = features.std(dim=(0, 2, 3))\n    return mean, std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:15.374586Z","iopub.execute_input":"2024-12-18T17:00:15.374935Z","iopub.status.idle":"2024-12-18T17:00:15.379606Z","shell.execute_reply.started":"2024-12-18T17:00:15.374905Z","shell.execute_reply":"2024-12-18T17:00:15.378676Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Напишите функцию get_augmentations, которая будет возвращать готовые аугментации для обучающей выборки и для тестовой выборки. Она должна иметь следующую сигнатуру: def get_augmentations(train: bool = True) -> T.Compose:.\n\nПримените следующие аугментации:\n\n Измените размер изображения, чтобы его размер был 224 на 224 пикселя (и для обучающей и для тестовой выборки).\n Примените какие-нибудь аугментации из тех, что мы изучили на занятии (только для обучающей).\n Преобразуйте картинку в тензор.\n Примените нормализацию для датасета CIFAR10\nПонятно, что изменение размера к 224 на 224 и нормализация для CIFAR10 вместе сочетаются плохо. Поэтому в дальнейшем, когда будете использовать эту функцию для получения аугментаций для вашего нового датасета, не забудьте поменять их на специфичные ему.","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms as T\n\ndef get_augmentations(train: bool = True) -> T.Compose:\n    \n    means = (0.49139968, 0.48215841, 0.44653091)\n    stds = (0.24703223, 0.24348513, 0.26158784)\n    \n    if train:\n        return T.Compose([\n            T.Resize((32, 32)),  # Изменение размера изображения\n            T.RandomAdjustSharpness(sharpness_factor=2),  # Изменение резкости\n            T.RandomHorizontalFlip(),  # Случайное горизонтальное отражение\n            T.RandomRotation(15),  # Случайный поворот на ±15 градусов\n            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Изменение цвета\n            T.ToTensor(),  # Преобразование в тензор\n            T.Normalize(means, stds)  # Нормализация\n        ])\n    else:\n        return T.Compose([\n            T.Resize((224, 224)),  # Изменение размера изображения\n            T.ToTensor(),  # Преобразование в тензор\n            T.Normalize(means, stds)  # Нормализация\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:16.901564Z","iopub.execute_input":"2024-12-18T17:00:16.901914Z","iopub.status.idle":"2024-12-18T17:00:17.967822Z","shell.execute_reply.started":"2024-12-18T17:00:16.901886Z","shell.execute_reply":"2024-12-18T17:00:17.967197Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Напишите функцию predict. Она должна принимать на вход нейронную сеть, даталоадер и torch.device. Она должна иметь следующую сигнатуру: def predict(model: nn.Module, loader: DataLoader, device: torch.device):\n\nВнутри функции сделайте следующие шаги:\n\nСоздайте пустой список для хранения предсказаний.\nПроитерируйтесь по даталоадеру.\nНа каждой итерации сделайте forward pass для батча, посчитайте классы как аргмакс по выходу нейросети, по логитам, добавьте тензор с предсказаниями в список.\nСделайте конкатенацию всех предсказаний и верните этот тензор длины N, по числу объектов в датасете.\nВаша функция должна возвращать тензор с классами.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\n@torch.inference_mode()\ndef predict(model: nn.Module, loader: DataLoader, device: torch.device) -> torch.Tensor:\n    predictions = []\n    model.eval()\n    \n    for (x, y) in loader:\n        output = model(x)\n        predict_classes = output.argmax(dim=1)\n        predictions.append(predict_classes)\n        \n    return torch.cat(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:19.324869Z","iopub.execute_input":"2024-12-18T17:00:19.325305Z","iopub.status.idle":"2024-12-18T17:00:19.330752Z","shell.execute_reply.started":"2024-12-18T17:00:19.325274Z","shell.execute_reply":"2024-12-18T17:00:19.329837Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Напишите функцию predict_tta. Она должна принимать на вход нейронную сеть, даталоадер, torch.device и количество итераций по даталоадеру. Она должна иметь следующую сигнатуру: def predict_tta(model: nn.Module, loader: DataLoader, device: torch.device, iterations: int = 2):.\n\nВ этой функции мы применим технику, которую кратко обсуждали на занятии  — Test Time Augmentation. Основная идея заключается в том, чтобы сделать для каждой картинки из тестовой выборки несколько аугментированных вариантов и сделать для них предсказания. Потом эти предсказания усреднить и использовать как обычно. В синтаксисе PyTorch это вырождается в создание тестового датасета со случайными аугментациями (либо как на обучающей выборке, либо чуть более слабыми). После этого нужно проитерироваться по созданному датасету несколько раз и усреднить ответы модели.\n\nВнутри функции сделайте следующие шаги:\n\nЗапустите цикл по количеству итераций.\nВнутри цикла проитерируйтесь по даталоадеру.\nЗапишите ответы (не классы, а сырые выходы нейросети) модели в один большой тензор размера [N, C], где C — число классов, а N — количество объектов в датасете (то есть мы должны иметь для каждого объекта вектор из выходов нейросети, логиты).\nСделайте из этих тензоров один огромный тензор размера [N, C, iterations], усредните его по итерациям, чтобы его размер стал [N, C]\nДальше предскажите классы для объектов по этому тензору как аргмакс, верните их из функции.\nВаша функция должна возвращать тензор с классами. Не забудьте переводить модель в режим применения и навешивать декторатор для выключения подсчета градиентов.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as Optimizer\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import CIFAR10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:21.606873Z","iopub.execute_input":"2024-12-18T17:00:21.607587Z","iopub.status.idle":"2024-12-18T17:00:21.611973Z","shell.execute_reply.started":"2024-12-18T17:00:21.607555Z","shell.execute_reply":"2024-12-18T17:00:21.611012Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\n@torch.inference_mode()\ndef predict_tta(model: nn.Module, loader: DataLoader, device: torch.device, iterations: int = 2):\n    model.eval()\n    all_logits = []\n    \n    for i in range(iterations):\n        logits_iter = []\n        \n        for images, _ in loader: # так как возвращает кортеж изображения и метки\n            output = model(images)\n            logits_iter.append(output)\n            \n        logits_iter = torch.cat(logits_iter, dim=0)\n        all_logits.append(logits_iter)\n\n    all_logits = torch.stack(all_logits, dim=-1) # Размерность [N, C, iterations]\n    avg_logits = all_logits.mean(dim=-1) # Размерность [N, C]\n\n    predictions_classes = avg_logits.argmax(dim=1) # Выбираем класс с максимальной вероятностью\n\n    return predictions_classes\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:24.021309Z","iopub.execute_input":"2024-12-18T17:00:24.022094Z","iopub.status.idle":"2024-12-18T17:00:24.027941Z","shell.execute_reply.started":"2024-12-18T17:00:24.022060Z","shell.execute_reply":"2024-12-18T17:00:24.027078Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def evaluate(model: nn.Module, data_loader: DataLoader, loss_fn) -> tuple[float, float]:\n    model.eval()\n\n    total_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(data_loader, desc='Evaluate'):\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        total_loss += loss.item()\n\n        _, y_pred = torch.max(output, 1)\n        total += y.size(0)\n        correct += (y_pred == y).sum().item()\n\n    total_loss /= len(loader)\n    accuracy = correct / total\n\n    return total_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:26.256368Z","iopub.execute_input":"2024-12-18T17:00:26.256964Z","iopub.status.idle":"2024-12-18T17:00:26.262616Z","shell.execute_reply.started":"2024-12-18T17:00:26.256930Z","shell.execute_reply":"2024-12-18T17:00:26.261706Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#!g1.1\nfrom tqdm import tqdm\n\n\ndef train(model: nn.Module, data_loader: DataLoader, optimizer: Optimizer, loss_fn) -> tuple[float, float]:\n    model.train()\n\n    train_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(train_loader, desc='Train'):\n        optimizer.zero_grad()\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        train_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n        \n        _, y_pred = torch.max(output, 1)\n        total += y.size(0)\n        correct += (y_pred == y).sum().item()\n\n    train_loss /= len(train_loader)\n    accuracy = correct / total\n\n    return train_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:00:29.972639Z","iopub.execute_input":"2024-12-18T17:00:29.973003Z","iopub.status.idle":"2024-12-18T17:00:29.979097Z","shell.execute_reply.started":"2024-12-18T17:00:29.972971Z","shell.execute_reply":"2024-12-18T17:00:29.978239Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class FirstModel(nn.Module):\n    def __init__(self):\n        super(FirstModel, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),  # Уменьшение размеров вдвое\n            \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2)  # Еще раз уменьшаем размеры вдвое\n        )\n        \n        # Расчет размерности выхода после сверточных слоев\n        self.flatten_size = self._get_flatten_size()\n\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.flatten_size, 128),  # Указываем рассчитанную размерность\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10)\n        )\n\n    def _get_flatten_size(self):\n        # Прогоняем фиктивный тензор через сверточные слои для вычисления выходного размера\n        with torch.no_grad():\n            x = torch.zeros(1, 3, 32, 32)  # CIFAR10 имеет размер 32x32 с 3 каналами\n            x = self.conv_layers(x)\n            return x.numel()  # Возвращаем общее количество элементов в выходном тензоре\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:28:10.013441Z","iopub.execute_input":"2024-12-18T17:28:10.013827Z","iopub.status.idle":"2024-12-18T17:28:10.021825Z","shell.execute_reply.started":"2024-12-18T17:28:10.013797Z","shell.execute_reply":"2024-12-18T17:28:10.020830Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nfrom torch.utils.data import DataLoader\n\n# Датасеты с аугментациями\ntrain_dataset = CIFAR10(\n    root='./data',\n    train=True,\n    download=True,\n    transform=get_augmentations(train=True)\n)\n\ntest_dataset = CIFAR10(\n    root='./data',\n    train=False,\n    download=True,\n    transform=get_augmentations(train=False)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:28:11.404693Z","iopub.execute_input":"2024-12-18T17:28:11.405076Z","iopub.status.idle":"2024-12-18T17:28:13.027189Z","shell.execute_reply.started":"2024-12-18T17:28:11.405045Z","shell.execute_reply":"2024-12-18T17:28:13.026234Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from torch.optim import Adam\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\nmodel = first_model()\n\noptimizer = Adam(model.parameters(), lr=0.0005)\nloss_fn = nn.CrossEntropyLoss()\n\nfor epoch in range(30):\n    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_fn)\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy}\")\n\ntest_loss, test_accuracy = evaluate(model, test_loader, loss_fn)\nprint(f\"Test Loss = {test_loss}, Test Accuracy = {test_accuracy}\")\n\n# Предсказания на тестовой выборке с использованием TTA\npredictions = predict_tta(model, test_loader, device, iterations=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T17:28:13.028923Z","iopub.execute_input":"2024-12-18T17:28:13.029533Z","iopub.status.idle":"2024-12-18T17:50:36.138840Z","shell.execute_reply.started":"2024-12-18T17:28:13.029490Z","shell.execute_reply":"2024-12-18T17:50:36.137367Z"}},"outputs":[{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 1.5436, Train Accuracy = 0.44338\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 18.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 1.2356, Train Accuracy = 0.56178\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 1.1336, Train Accuracy = 0.60176\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss = 1.0653, Train Accuracy = 0.62766\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss = 1.0238, Train Accuracy = 0.64306\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss = 0.9859, Train Accuracy = 0.656\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:45<00:00, 17.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss = 0.9573, Train Accuracy = 0.66848\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss = 0.9372, Train Accuracy = 0.6746\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss = 0.9099, Train Accuracy = 0.684\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 0.8919, Train Accuracy = 0.68838\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss = 0.8805, Train Accuracy = 0.69234\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss = 0.8619, Train Accuracy = 0.70042\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss = 0.8520, Train Accuracy = 0.70402\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss = 0.8352, Train Accuracy = 0.7091\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss = 0.8275, Train Accuracy = 0.71352\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss = 0.8214, Train Accuracy = 0.71574\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss = 0.8014, Train Accuracy = 0.72442\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss = 0.7977, Train Accuracy = 0.72456\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss = 0.7894, Train Accuracy = 0.7234\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss = 0.7781, Train Accuracy = 0.73052\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss = 0.7718, Train Accuracy = 0.73176\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss = 0.7620, Train Accuracy = 0.73586\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:42<00:00, 18.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss = 0.7568, Train Accuracy = 0.73718\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:44<00:00, 17.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss = 0.7490, Train Accuracy = 0.73942\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:43<00:00, 17.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss = 0.7488, Train Accuracy = 0.74042\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:46<00:00, 16.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss = 0.7355, Train Accuracy = 0.74458\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:46<00:00, 16.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss = 0.7306, Train Accuracy = 0.74496\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:48<00:00, 15.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss = 0.7347, Train Accuracy = 0.74614\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:47<00:00, 16.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss = 0.7241, Train Accuracy = 0.74742\n","output_type":"stream"},{"name":"stderr","text":"Train: 100%|██████████| 782/782 [00:48<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss = 0.7196, Train Accuracy = 0.75132\n","output_type":"stream"},{"name":"stderr","text":"Evaluate:   0%|          | 0/157 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, loss_fn)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Предсказания на тестовой выборке с использованием TTA\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, loss_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluate\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[1;32m     13\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mfirst_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers(x)\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x200704 and 4096x128)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (64x200704 and 4096x128)","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# Сохранение предсказаний\ntorch.save(predictions, \"cifar10_predictions.pt\")\nprint(\"Predictions saved to cifar10_predictions.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms as T\n\n# Функция создания модели\ndef create_simple_conv_cifar():\n    class SimpleConvNet(nn.Module):\n        def __init__(self):\n            super(SimpleConvNet, self).__init__()\n            self.conv_layers = nn.Sequential(\n                nn.Conv2d(3, 32, kernel_size=3, padding=1),\n                nn.ReLU(),\n                nn.BatchNorm2d(32),\n                nn.MaxPool2d(2, 2),  # Сжатие до половины\n                \n                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n                nn.ReLU(),\n                nn.BatchNorm2d(64),\n                nn.MaxPool2d(2, 2)  # Снова сжатие до половины\n            )\n            \n            # Автоматический расчет выходного размера\n            self.flatten_size = self._get_flatten_size()\n            \n            self.fc_layers = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(self.flatten_size, 128),\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(128, 10)\n            )\n        \n        def _get_flatten_size(self):\n            with torch.no_grad():\n                x = torch.zeros(1, 3, 32, 32)\n                x = self.conv_layers(x)\n                return x.numel()\n\n        def forward(self, x):\n            x = self.conv_layers(x)\n            x = self.fc_layers(x)\n            return x\n\n    return SimpleConvNet()\n\n# Аугментации для CIFAR10\ndef get_augmentations(train=True):\n    means = (0.49139968, 0.48215841, 0.44653091)\n    stds = (0.24703223, 0.24348513, 0.26158784)\n    if train:\n        return T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomCrop(32, padding=4),\n            T.ToTensor(),\n            T.Normalize(means, stds)\n        ])\n    else:\n        return T.Compose([\n            T.ToTensor(),\n            T.Normalize(means, stds)\n        ])\n\n# Обучение\ndef train(model, loader, optimizer, loss_fn, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    return running_loss / len(loader), correct / total\n\n# Оценка модели\ndef evaluate(model, loader, loss_fn, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = loss_fn(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return running_loss / len(loader), correct / total\n\n# Функция TTA\n@torch.inference_mode()\ndef predict_tta(model, loader, device, iterations=3):\n    model.eval()\n    all_logits = []\n\n    for _ in range(iterations):\n        logits_iter = []\n        for images, _ in loader:\n            images = images.to(device)\n            outputs = model(images)\n            logits_iter.append(outputs.cpu())\n        logits_iter = torch.cat(logits_iter, dim=0)\n        all_logits.append(logits_iter)\n\n    all_logits = torch.stack(all_logits, dim=-1)  # [N, C, iterations]\n    avg_logits = all_logits.mean(dim=-1)  # [N, C]\n    predicted_classes = avg_logits.argmax(dim=1)\n\n    return predicted_classes\n\n# Загрузка данных\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=get_augmentations(train=True))\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=get_augmentations(train=False))\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# Установка устройства\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Инициализация модели, оптимизатора, функции потерь\nmodel = create_simple_conv_cifar().to(device)\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n\n# Обучение модели\nfor epoch in range(30):\n    train_loss, train_accuracy = train(model, train_loader, optimizer, loss_fn, device)\n    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.2%}\")\n\n    test_loss, test_accuracy = evaluate(model, test_loader, loss_fn, device)\n    print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.2%}\")\n\n# Предсказания на тестовой выборке с использованием TTA\npredictions = predict_tta(model, test_loader, device, iterations=5)\n\n# Сохранение предсказаний\ntorch.save(predictions, \"cifar10_predictions.pt\")\nprint(\"Predictions saved to cifar10_predictions.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T11:12:10.058288Z","iopub.execute_input":"2024-12-19T11:12:10.058779Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 67475303.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nEpoch 1: Train Loss = 1.6184, Train Accuracy = 41.08%\nTest Loss = 1.1733, Test Accuracy = 57.38%\nEpoch 2: Train Loss = 1.3355, Train Accuracy = 51.98%\nTest Loss = 1.0158, Test Accuracy = 63.84%\nEpoch 3: Train Loss = 1.2235, Train Accuracy = 56.61%\nTest Loss = 0.9466, Test Accuracy = 66.88%\nEpoch 4: Train Loss = 1.1491, Train Accuracy = 59.31%\nTest Loss = 0.8941, Test Accuracy = 68.07%\nEpoch 5: Train Loss = 1.0992, Train Accuracy = 61.39%\nTest Loss = 0.8351, Test Accuracy = 71.42%\nEpoch 6: Train Loss = 1.0599, Train Accuracy = 62.79%\nTest Loss = 0.8047, Test Accuracy = 71.97%\nEpoch 7: Train Loss = 1.0380, Train Accuracy = 64.06%\nTest Loss = 0.7801, Test Accuracy = 72.49%\nEpoch 8: Train Loss = 1.0101, Train Accuracy = 64.99%\nTest Loss = 0.7838, Test Accuracy = 73.19%\nEpoch 9: Train Loss = 0.9858, Train Accuracy = 65.58%\nTest Loss = 0.7611, Test Accuracy = 73.55%\nEpoch 10: Train Loss = 0.9669, Train Accuracy = 66.45%\nTest Loss = 0.7414, Test Accuracy = 74.36%\nEpoch 11: Train Loss = 0.9371, Train Accuracy = 67.44%\nTest Loss = 0.7392, Test Accuracy = 74.13%\nEpoch 12: Train Loss = 0.9252, Train Accuracy = 68.08%\nTest Loss = 0.7409, Test Accuracy = 74.59%\n","output_type":"stream"}],"execution_count":null}]}